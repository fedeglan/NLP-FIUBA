{"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## LSTM Bot QA"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### Datos\n","El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n","[LINK](http://convai.io/data/)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bDFC0I3j9oFD","executionInfo":{"status":"ok","timestamp":1695666592944,"user_tz":180,"elapsed":7231,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown --quiet"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"cq3YXak9sGHd","executionInfo":{"status":"ok","timestamp":1695666598425,"user_tz":180,"elapsed":5485,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"outputs":[],"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from keras.preprocessing.text import one_hot\n","from tensorflow.keras.utils import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Activation, Dropout, Dense\n","from keras.layers import Flatten, LSTM, SimpleRNN\n","from keras.models import Model\n","from tensorflow.keras.layers import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RHNkUaPp6aYq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695666599844,"user_tz":180,"elapsed":1426,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"d67fc1a5-a103-4a29-c9ff-ef8bdb6bf01c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download\n","To: /content/data_volunteers.json\n","100%|██████████| 2.58M/2.58M [00:00<00:00, 144MB/s]\n"]}],"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('data_volunteers.json', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n","    output = 'data_volunteers.json'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WZy1-wgG-Rp7","executionInfo":{"status":"ok","timestamp":1695666614901,"user_tz":180,"elapsed":527,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"outputs":[],"source":["# dataset_file\n","import json\n","\n","text_file = \"data_volunteers.json\"\n","with open(text_file) as f:\n","    data = json.load(f) # la variable data será un diccionario\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ue5qd54S-eew","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695666615644,"user_tz":180,"elapsed":2,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"a7899143-d3df-4fdf-e13f-bff326042d11"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"]},"metadata":{},"execution_count":5}],"source":["# Observar los campos disponibles en cada linea del dataset\n","data[0].keys()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jHBRAXPl-3dz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695666617325,"user_tz":180,"elapsed":5,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"ae05033f-e43e-4edc-ed4d-26943bd2c09e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad de rows utilizadas: 6033\n"]}],"source":["chat_in = []\n","chat_out = []\n","\n","input_sentences = []\n","output_sentences = []\n","output_sentences_inputs = []\n","max_len = 30\n","\n","def clean_text(txt):\n","    txt = txt.lower()\n","    txt.replace(\"\\'d\", \" had\")\n","    txt.replace(\"\\'s\", \" is\")\n","    txt.replace(\"\\'m\", \" am\")\n","    txt.replace(\"don't\", \"do not\")\n","    txt = re.sub(r'\\W+', ' ', txt)\n","\n","    return txt\n","\n","for line in data:\n","    for i in range(len(line['dialog'])-1):\n","        # vamos separando el texto en \"preguntas\" (chat_in)\n","        # y \"respuestas\" (chat_out)\n","        chat_in = clean_text(line['dialog'][i]['text'])\n","        chat_out = clean_text(line['dialog'][i+1]['text'])\n","\n","        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n","            continue\n","\n","        input_sentence, output = chat_in, chat_out\n","\n","        # output sentence (decoder_output) tiene <eos>\n","        output_sentence = output + ' <eos>'\n","        # output sentence input (decoder_input) tiene <sos>\n","        output_sentence_input = '<sos> ' + output\n","\n","        input_sentences.append(input_sentence)\n","        output_sentences.append(output_sentence)\n","        output_sentences_inputs.append(output_sentence_input)\n","\n","print(\"Cantidad de rows utilizadas:\", len(input_sentences))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"07L1qj8pC_l6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695666622138,"user_tz":180,"elapsed":7,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"e2716c05-0c3e-49fc-8a2f-56fc74a7b37b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"]},"metadata":{},"execution_count":7}],"source":["input_sentences[1], output_sentences[1], output_sentences_inputs[1]"]},{"cell_type":"markdown","metadata":{"id":"8P-ynUNP5xp6"},"source":["### 2 - Preprocesamiento\n","Realizar el preprocesamiento necesario para obtener:\n","- word2idx_inputs, max_input_len\n","- word2idx_outputs, max_out_len, num_words_output\n","- encoder_input_sequences, decoder_output_sequences, decoder_targets"]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Tokenización\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_sentences)\n","input_sequences = input_tokenizer.texts_to_sequences(input_sentences)\n","word2idx_inputs = input_tokenizer.word_index\n","\n","output_tokenizer = Tokenizer(filters='')\n","output_tokenizer.fit_on_texts(output_sentences)\n","output_sequences = output_tokenizer.texts_to_sequences(output_sentences)\n","word2idx_outputs = output_tokenizer.word_index\n","\n","# Determinación de la Longitud Máxima de Secuencia\n","max_input_len = max(len(s) for s in input_sequences)\n","max_out_len = max(len(s) for s in output_sequences)\n","\n","# Número de palabras únicas en el output\n","num_words_output = len(word2idx_outputs) + 1\n","\n","# Preparación de Secuencias de Encoder y Decoder\n","encoder_input_sequences = pad_sequences(input_sequences, maxlen=max_input_len)\n","decoder_output_sequences = pad_sequences(output_sequences, maxlen=max_out_len, padding='post')\n","\n","# Preparación de targets para el decoder\n","decoder_targets = np.zeros((len(input_sentences), max_out_len, num_words_output), dtype='float32')\n","for i, seq in enumerate(decoder_output_sequences):\n","    for j, word_idx in enumerate(seq):\n","        if word_idx > 0:\n","            decoder_targets[i, j, word_idx] = 1.\n"],"metadata":{"id":"koKIJYbynhGw","executionInfo":{"status":"ok","timestamp":1695668036869,"user_tz":180,"elapsed":555,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_CJIsLBbj6rg"},"source":["### 3 - Preparar los embeddings\n","Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"]},{"cell_type":"code","source":["import os\n","import gdown\n","if os.access('gloveembedding.pkl', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download'\n","    output = 'gloveembedding.pkl'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGGadfHvoQMw","executionInfo":{"status":"ok","timestamp":1695667005545,"user_tz":180,"elapsed":5651,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"0334e6ea-5154-4bc6-be38-e1d1db2069ee"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (uriginal): https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download\n","From (redirected): https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download&confirm=t&uuid=d4c19f4a-61e5-48ed-8859-9e02d9bbaa38\n","To: /content/gloveembedding.pkl\n","100%|██████████| 525M/525M [00:04<00:00, 120MB/s]\n"]}]},{"cell_type":"code","source":["# Cargar los embeddings de Glove\n","import pickle\n","import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, Embedding\n","\n","# Carga los embeddings de Glove\n","with open('gloveembedding.pkl', 'rb') as f:\n","    embeddings_index = pickle.load(f)"],"metadata":{"id":"0fbQP2OXn0we","executionInfo":{"status":"ok","timestamp":1695667087940,"user_tz":180,"elapsed":788,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import logging\n","import os\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","class WordsEmbeddings(object):\n","    logger = logging.getLogger(__name__)\n","\n","    def __init__(self):\n","        # load the embeddings\n","        words_embedding_pkl = Path(self.PKL_PATH)\n","        if not words_embedding_pkl.is_file():\n","            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n","            assert words_embedding_txt.is_file(), 'Words embedding not available'\n","            embeddings = self.convert_model_to_pickle()\n","        else:\n","            embeddings = self.load_model_from_pickle()\n","        self.embeddings = embeddings\n","        # build the vocabulary hashmap\n","        index = np.arange(self.embeddings.shape[0])\n","        # Dicctionarios para traducir de embedding a IDX de la palabra\n","        self.word2idx = dict(zip(self.embeddings['word'], index))\n","        self.idx2word = dict(zip(index, self.embeddings['word']))\n","\n","    def get_words_embeddings(self, words):\n","        words_idxs = self.words2idxs(words)\n","        return self.embeddings[words_idxs]['embedding']\n","\n","    def words2idxs(self, words):\n","        return np.array([self.word2idx.get(word, -1) for word in words])\n","\n","    def idxs2words(self, idxs):\n","        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n","\n","    def load_model_from_pickle(self):\n","        self.logger.debug(\n","            'loading words embeddings from pickle {}'.format(\n","                self.PKL_PATH\n","            )\n","        )\n","        max_bytes = 2**28 - 1 # 256MB\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(self.PKL_PATH)\n","        with open(self.PKL_PATH, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","        embeddings = pickle.loads(bytes_in)\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","    def convert_model_to_pickle(self):\n","        # create a numpy strctured array:\n","        # word     embedding\n","        # U50      np.float32[]\n","        # word_1   a, b, c\n","        # word_2   d, e, f\n","        # ...\n","        # word_n   g, h, i\n","        self.logger.debug(\n","            'converting and loading words embeddings from text file {}'.format(\n","                self.WORD_TO_VEC_MODEL_TXT_PATH\n","            )\n","        )\n","        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n","                     ('embedding', np.float32, (self.N_FEATURES,))]\n","        structure = np.dtype(structure)\n","        # load numpy array from disk using a generator\n","        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n","            embeddings_gen = (\n","                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n","                if len(line.split()[1:]) == self.N_FEATURES\n","            )\n","            embeddings = np.fromiter(embeddings_gen, structure)\n","        # add a null embedding\n","        null_embedding = np.array(\n","            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n","            dtype=structure\n","        )\n","        embeddings = np.concatenate([embeddings, null_embedding])\n","        # dump numpy array to disk using pickle\n","        max_bytes = 2**28 - 1 # # 256MB\n","        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n","        with open(self.PKL_PATH, 'wb') as f_out:\n","            for idx in range(0, len(bytes_out), max_bytes):\n","                f_out.write(bytes_out[idx:idx+max_bytes])\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","class GloveEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n","    PKL_PATH = 'gloveembedding.pkl'\n","    N_FEATURES = 50\n","    WORD_MAX_SIZE = max_input_len\n","\n","class FasttextEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n","    PKL_PATH = 'fasttext.pkl'\n","    N_FEATURES = 300\n","    WORD_MAX_SIZE = 60"],"metadata":{"id":"0fT54xrNoqu4","executionInfo":{"status":"ok","timestamp":1695668074108,"user_tz":180,"elapsed":325,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["model_embeddings = GloveEmbeddings()"],"metadata":{"id":"KLAzRL5ipjdA","executionInfo":{"status":"ok","timestamp":1695668077707,"user_tz":180,"elapsed":3105,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# Crear la Embedding matrix de las secuencias\n","print('preparing embedding matrix...')\n","MAX_VOCAB_SIZE = 8000\n","embed_dim = model_embeddings.N_FEATURES\n","words_not_found = []\n","\n","# word_index provieen del tokenizer\n","\n","nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word2idx_inputs.items():\n","    if i >= nb_words:\n","        continue\n","    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # words not found in embedding index will be all-zeros.\n","        words_not_found.append(word)\n","\n","print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDuPjZ9zpoIQ","executionInfo":{"status":"ok","timestamp":1695668077707,"user_tz":180,"elapsed":4,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"a2a3374e-f9cc-4189-a506-9b2585458c21"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["preparing embedding matrix...\n","number of null word embeddings: 38\n"]}]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 4 - Entrenar el modelo\n","Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."]},{"cell_type":"code","source":["max_input_len"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dEv5_1csLYf","executionInfo":{"status":"ok","timestamp":1695668081814,"user_tz":180,"elapsed":330,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"dd0c3442-8815-42c0-f89d-5336b09434b6"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","\n","n_units = 128\n","encoder_inputs = Input(shape=(max_input_len))\n","encoder_embedding_layer = Embedding(\n","          input_dim=nb_words,\n","          output_dim=embed_dim,\n","          input_length=max_input_len,\n","          weights=[embedding_matrix],\n","          trainable=False)\n","\n","encoder_inputs_x = encoder_embedding_layer(encoder_inputs)\n","encoder = LSTM(n_units, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs_x)\n","encoder_states = [state_h, state_c]\n","decoder_inputs = Input(shape=(max_out_len))\n","decoder_embedding_layer = Embedding(input_dim=num_words_output, output_dim=n_units, input_length=max_out_len)\n","decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n","decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n","decoder_dense = Dense(num_words_output, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZERSmOJp67L","executionInfo":{"status":"ok","timestamp":1695668086480,"user_tz":180,"elapsed":1538,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"f39501a1-94b2-43e3-8390-b72ecc93c35d"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_9 (InputLayer)        [(None, 9)]                  0         []                            \n","                                                                                                  \n"," input_10 (InputLayer)       [(None, 10)]                 0         []                            \n","                                                                                                  \n"," embedding_5 (Embedding)     (None, 9, 50)                89950     ['input_9[0][0]']             \n","                                                                                                  \n"," embedding_6 (Embedding)     (None, 10, 128)              231168    ['input_10[0][0]']            \n","                                                                                                  \n"," lstm_4 (LSTM)               [(None, 128),                91648     ['embedding_5[0][0]']         \n","                              (None, 128),                                                        \n","                              (None, 128)]                                                        \n","                                                                                                  \n"," lstm_5 (LSTM)               [(None, 10, 128),            131584    ['embedding_6[0][0]',         \n","                              (None, 128),                           'lstm_4[0][1]',              \n","                              (None, 128)]                           'lstm_4[0][2]']              \n","                                                                                                  \n"," dense_2 (Dense)             (None, 10, 1806)             232974    ['lstm_5[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 777324 (2.97 MB)\n","Trainable params: 687374 (2.62 MB)\n","Non-trainable params: 89950 (351.37 KB)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Encoder Model\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# Decoder Model\n","decoder_state_input_h = Input(shape=(n_units,))\n","decoder_state_input_c = Input(shape=(n_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_inputs_single = Input(shape=(1,))\n","decoder_inputs_single_x = decoder_embedding_layer(decoder_inputs_single)\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)"],"metadata":{"id":"qhW83qH3qKfH","executionInfo":{"status":"ok","timestamp":1695668089416,"user_tz":180,"elapsed":427,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# Entrenamiento\n","hist = model.fit(\n","    [encoder_input_sequences, decoder_output_sequences],  # Inputs del modelo\n","    decoder_targets,  # Targets\n","    epochs=15,\n","    validation_split=0.2\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_Fdo79iqWpv","executionInfo":{"status":"ok","timestamp":1695668177694,"user_tz":180,"elapsed":86661,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"e2a5ebf1-b197-4cf3-db92-01e7fcc99513"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","151/151 [==============================] - 20s 63ms/step - loss: 2.4511 - accuracy: 0.1178 - val_loss: 2.2827 - val_accuracy: 0.1289\n","Epoch 2/15\n","151/151 [==============================] - 3s 19ms/step - loss: 2.1756 - accuracy: 0.1429 - val_loss: 2.1833 - val_accuracy: 0.1656\n","Epoch 3/15\n","151/151 [==============================] - 2s 15ms/step - loss: 2.0463 - accuracy: 0.1786 - val_loss: 2.0784 - val_accuracy: 0.1709\n","Epoch 4/15\n","151/151 [==============================] - 2s 14ms/step - loss: 1.9407 - accuracy: 0.1866 - val_loss: 2.0031 - val_accuracy: 0.1738\n","Epoch 5/15\n","151/151 [==============================] - 3s 17ms/step - loss: 1.8598 - accuracy: 0.1961 - val_loss: 1.9385 - val_accuracy: 0.1942\n","Epoch 6/15\n","151/151 [==============================] - 3s 18ms/step - loss: 1.7741 - accuracy: 0.2133 - val_loss: 1.8399 - val_accuracy: 0.2064\n","Epoch 7/15\n","151/151 [==============================] - 3s 17ms/step - loss: 1.6728 - accuracy: 0.2240 - val_loss: 1.7634 - val_accuracy: 0.2199\n","Epoch 8/15\n","151/151 [==============================] - 2s 12ms/step - loss: 1.5834 - accuracy: 0.2443 - val_loss: 1.6733 - val_accuracy: 0.2308\n","Epoch 9/15\n","151/151 [==============================] - 2s 13ms/step - loss: 1.4803 - accuracy: 0.2560 - val_loss: 1.5736 - val_accuracy: 0.2391\n","Epoch 10/15\n","151/151 [==============================] - 2s 13ms/step - loss: 1.3709 - accuracy: 0.2668 - val_loss: 1.4728 - val_accuracy: 0.2628\n","Epoch 11/15\n","151/151 [==============================] - 3s 21ms/step - loss: 1.2717 - accuracy: 0.2894 - val_loss: 1.3891 - val_accuracy: 0.2799\n","Epoch 12/15\n","151/151 [==============================] - 2s 14ms/step - loss: 1.1762 - accuracy: 0.3187 - val_loss: 1.3051 - val_accuracy: 0.3037\n","Epoch 13/15\n","151/151 [==============================] - 2s 14ms/step - loss: 1.0764 - accuracy: 0.3420 - val_loss: 1.2179 - val_accuracy: 0.3268\n","Epoch 14/15\n","151/151 [==============================] - 2s 13ms/step - loss: 0.9777 - accuracy: 0.3691 - val_loss: 1.1313 - val_accuracy: 0.3476\n","Epoch 15/15\n","151/151 [==============================] - 2s 12ms/step - loss: 0.8842 - accuracy: 0.3872 - val_loss: 1.0559 - val_accuracy: 0.3576\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zbwn0ekDy_s2"},"source":["### 5 - Inferencia\n","Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."]},{"cell_type":"code","source":["output_tokenizer = Tokenizer(filters='', oov_token='<sos>')\n","idx2word_outputs = {v: k for k, v in word2idx_outputs.items()}\n","if '<sos>' not in idx2word_outputs.values():\n","    idx2word_outputs[len(idx2word_outputs) + 1] = '<sos>'\n","if '<sos>' not in word2idx_outputs:\n","    word2idx_outputs['<sos>'] = len(word2idx_outputs) + 1"],"metadata":{"id":"AYzzZaRWtiK6","executionInfo":{"status":"ok","timestamp":1695668399556,"user_tz":180,"elapsed":445,"user":{"displayName":"Federico","userId":"07235782102280597256"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def respond(input_text):\n","    # Preprocesamiento de la entrada\n","    input_text = clean_text(input_text)\n","    input_seq = input_tokenizer.texts_to_sequences([input_text])\n","    input_seq = pad_sequences(input_seq, maxlen=max_input_len)\n","\n","    # Encodear la secuencia de entrada\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Crear una secuencia de longitud 1 para el token <sos>\n","    target_seq = np.zeros((1, 1))\n","    target_seq[0, 0] = word2idx_outputs['<sos>']\n","\n","    # Recoger la respuesta\n","    output_sentence = []\n","\n","    for _ in range(max_out_len):\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Elegir el siguiente token\n","        idx = np.argmax(output_tokens[0, 0, :])\n","\n","        # Si es <eos>, terminar la generación\n","        if idx == word2idx_outputs['<eos>']:\n","            break\n","\n","        # Añadir el token a la respuesta\n","        if idx > 0:\n","            output_sentence.append(idx2word_outputs[idx])\n","\n","        # Actualizar la secuencia de entrada del decoder y los estados\n","        target_seq[0, 0] = idx\n","        states_value = [h, c]\n","\n","    return ' '.join(output_sentence)\n","\n","# Crear un diccionario inverso para el output\n","idx2word_outputs = {v: k for k, v in word2idx_outputs.items()}\n","\n","# Usar la función respond para generar una respuesta\n","input_text = \"How are you?\"\n","print(respond(input_text))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWFlCYszsyT3","executionInfo":{"status":"ok","timestamp":1695668495247,"user_tz":180,"elapsed":1783,"user":{"displayName":"Federico","userId":"07235782102280597256"}},"outputId":"b91c1e45-5345-4abd-87b9-8fe8e6f08ddc"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 29ms/step\n","and and and and and and and and and and\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}